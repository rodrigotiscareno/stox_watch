{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b90ceea2-cbb3-4839-9629-cb8e3950d55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7de56c-4979-4df2-8b24-3a93519c632a",
   "metadata": {},
   "source": [
    "## Data Extracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b43d22-1b07-48d6-9c9a-d795dbc83c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(data):\n",
    "    data = data.json()\n",
    "    try:\n",
    "        df = pd.DataFrame(data['feed'])\n",
    "        df = df[[\n",
    "        'title',\n",
    "        'url',\n",
    "        'time_published',\n",
    "        'summary',\n",
    "        'source',\n",
    "        'category_within_source',\n",
    "        'source_domain',\n",
    "        'overall_sentiment_score',\n",
    "        'overall_sentiment_label',\n",
    "        'ticker_sentiment']]\n",
    "        df = pd.concat([df.explode('ticker_sentiment').drop(['ticker_sentiment'], axis=1),\n",
    "               df.explode('ticker_sentiment')['ticker_sentiment'].apply(pd.Series)],\n",
    "              axis=1)\n",
    "        df['time_published'] = pd.to_datetime(df['time_published'], format='%Y%m%dT%H%M%S')\n",
    "        return df \n",
    "    except:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5dc7a4-cb14-4e29-bf4e-2463ec4bfea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_snp_tickers()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "chunks = [tickers[i:i + 5] for i in range(0, len(tickers), 5)]\n",
    "\n",
    "for chunk in chunks:\n",
    "    for ticker in chunk:\n",
    "        print(ticker)\n",
    "        url = f'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={ticker}&apikey={TOKEN}'\n",
    "        r = requests.get(url)\n",
    "        df = pd.concat([df, get_df(r)], ignore_index=True)\n",
    "    time.sleep(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1003cf92-686c-4aa2-a2c3-cc99955ad6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/alpha_vantage_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00aace9-41b5-4ce1-bb15-e42f7832860a",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "240b253f-7b90-4eba-a3cd-b14fc16ba4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/alpha_vantage_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f8fdb7a-6048-4982-979b-3c44818892c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cbe4eda-e9e0-41a2-8bba-f64aab37290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_published'] = pd.to_datetime(df['time_published'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c5ab85-5bf5-40ed-a72c-d2fabc95a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e350610d-f14a-4541-8efa-2c40488bfe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla =df.query('ticker == \"TSLA\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ce88537-4501-4513-9a18-83e009acfce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp-plot.html'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as py\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "\n",
    "tsla = tsla.sort_values(by=\"time_published\")\n",
    "x = (tsla['time_published'] - tsla['time_published'].min()).dt.total_seconds()\n",
    "\n",
    "# y is the 'ticker_sentiment_score'\n",
    "y = tsla['ticker_sentiment_score']\n",
    "loess_smoothed = lowess(y, x, frac=0.1) # try adjusting frac to change the amount of smoothing\n",
    "\n",
    "y_moving_avg = y.rolling(window=7).mean()\n",
    "\n",
    "\n",
    "# calculate regression line\n",
    "slope, intercept = np.polyfit(x, y, 1)\n",
    "reg_line = slope*x + intercept\n",
    "\n",
    "fig = px.line(tsla, x=\"time_published\", y=\"ticker_sentiment_score\", title='$TSLA Over Time')\n",
    "fig.add_trace(go.Scatter(x=tsla['time_published'], y=loess_smoothed[:,1], mode='lines', name='LOESS Smoothed'))\n",
    "fig.add_trace(go.Scatter(x=tsla['time_published'], y=reg_line, mode='lines', name='Regression Line'))\n",
    "fig.add_trace(go.Scatter(x=tsla['time_published'], y=y_moving_avg, mode='lines', name='Moving Average'))\n",
    "\n",
    "# plot the figure\n",
    "py.plot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3bab6-2f7a-43c0-b2f6-2e7923be5322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87347c48-4245-4a48-a56a-a73ce5a52f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
